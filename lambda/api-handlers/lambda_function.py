import json
import boto3
import os
from datetime import datetime, timedelta
from decimal import Decimal
import time
import warnings
import requests

# Import NumPy from the Lambda layer
import numpy as np

# Suppress warnings for cleaner logs
warnings.filterwarnings('ignore')

# Initialize AWS clients
dynamodb = boto3.resource('dynamodb', region_name='ap-east-1')
lambda_client = boto3.client('lambda', region_name='ap-east-1')
current_state_table = dynamodb.Table('gym-pulse-current-state')
aggregates_table = dynamodb.Table('gym-pulse-aggregates')

# Google Gemini API configuration
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY', '')
GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

def format_machine_name(machine_id):
    """
    Convert machine ID to human-readable name
    e.g. 'calf-raise-01' -> 'Calf Raise'
    """
    if not machine_id:
        return 'Unknown Machine'

    # Remove numbers and dashes, convert to title case
    name = machine_id.replace('-', ' ')
    # Remove trailing numbers (e.g., '01', '02', '03')
    name = ' '.join(word for word in name.split() if not word.isdigit())
    return name.title()

def get_machine_type(machine_id):
    """
    Extract machine type from machine ID
    e.g. 'leg-press-01' -> 'leg-press'
    """
    if not machine_id:
        return 'unknown'

    # Remove trailing numbers to get the base type
    parts = machine_id.split('-')
    # Remove numeric parts from the end
    while parts and parts[-1].isdigit():
        parts.pop()

    return '-'.join(parts) if parts else 'unknown'

class MLForecastEngine:
    """
    AI-Powered Gym Equipment Forecasting Engine - Integrated into API Handler

    Features:
    1. Time Series Forecasting with Multiple Models
    2. Anomaly Detection for Unusual Patterns
    3. Google Gemini API Integration for AI Insights
    4. Real-time Model Updates
    5. Multi-feature Prediction (seasonality, trends, context)
    """

    def __init__(self):
        self.dynamodb = dynamodb
        self.events_table = None  # Not needed for this integration
        self.aggregates_table = aggregates_table
        self.current_state_table = current_state_table
        self.anomaly_threshold = 2.0  # Standard deviations for anomaly detection

    def generate_ai_forecast(self, machine_id, historical_data, current_context):
        """Main AI forecasting function combining multiple ML approaches"""
        print(f"ü§ñ Starting AI forecast generation for {machine_id}")

        try:
            # 1. Prepare time series data
            data_array = self.prepare_time_series_data(historical_data)

            if len(data_array) < 50:  # Minimum viable dataset
                return self.fallback_forecast(machine_id, current_context)

            # 2. Detect anomalies
            anomalies = self.detect_anomalies(data_array)

            # 3. Run ensemble prediction models
            ensemble_forecast = self.run_ensemble_models(data_array, current_context)

            # 4. Generate forecast with confidence intervals
            forecast_results = self.generate_forecast_with_confidence(ensemble_forecast, current_context)

            # 5. AI insights using Google Gemini API
            ai_insights = self.generate_gemini_insights(machine_id, data_array, ensemble_forecast, anomalies)

            # 6. Combine results
            final_forecast = {
                'forecast': forecast_results,
                'anomalies': anomalies,
                'ml_insights': ai_insights,
                'confidence_score': self.calculate_confidence_score(data_array, anomalies),
                'models_used': 4,
                'data_points_analyzed': len(data_array)
            }

            print(f"‚úÖ AI forecast completed for {machine_id}")
            return final_forecast

        except Exception as e:
            print(f"‚ùå Error in AI forecast: {str(e)}")
            return self.fallback_forecast(machine_id, current_context)

    def prepare_time_series_data(self, historical_data):
        """Convert historical data to NumPy arrays for ML processing"""
        if not historical_data:
            return np.array([])

        # Extract relevant features
        timestamps = []
        occupancy_ratios = []

        for record in historical_data:
            try:
                timestamps.append(int(record.get('timestamp15min', 0)))
                occupancy_ratios.append(float(record.get('occupancyRatio', 0)))
            except (ValueError, TypeError):
                continue

        if not timestamps:
            return np.array([])

        # Create structured array for time series analysis
        data = np.array([
            (ts, ratio) for ts, ratio in zip(timestamps, occupancy_ratios)
        ], dtype=[('timestamp', 'i8'), ('occupancy_ratio', 'f4')])

        # Sort by timestamp
        data = np.sort(data, order='timestamp')

        print(f"üìä Prepared {len(data)} data points for ML analysis")
        return data

    def detect_anomalies(self, data_array):
        """Detect unusual patterns using statistical analysis"""
        if len(data_array) < 20:
            return []

        try:
            occupancy_values = data_array['occupancy_ratio']

            # Calculate rolling statistics
            window_size = min(20, len(occupancy_values) // 4)
            rolling_mean = np.convolve(occupancy_values, np.ones(window_size)/window_size, mode='valid')
            rolling_std = np.array([
                np.std(occupancy_values[max(0, i-window_size):i+window_size])
                for i in range(len(occupancy_values))
            ])

            # Detect anomalies (values beyond threshold standard deviations)
            anomalies = []
            for i in range(len(occupancy_values)):
                if i < len(rolling_mean):
                    z_score = abs(occupancy_values[i] - rolling_mean[i]) / max(rolling_std[i], 1.0)
                    if z_score > self.anomaly_threshold:
                        anomalies.append({
                            'timestamp': int(data_array['timestamp'][i]),
                            'value': float(occupancy_values[i]),
                            'expected': float(rolling_mean[i]),
                            'severity': min(z_score / self.anomaly_threshold, 3.0)
                        })

            print(f"üîç Detected {len(anomalies)} anomalies")
            return anomalies

        except Exception as e:
            print(f"‚ùå Error in anomaly detection: {str(e)}")
            return []

    def run_ensemble_models(self, data_array, current_context):
        """Run multiple ML models and combine predictions"""
        models = {
            'seasonal_decomposition': self.seasonal_decomposition_model,
            'pattern_recognition': self.pattern_recognition_model,
            'trend_analysis': self.trend_analysis_model,
            'context_aware': self.context_aware_model
        }

        ensemble_results = {}
        weights = {'seasonal_decomposition': 0.3, 'pattern_recognition': 0.3, 'trend_analysis': 0.2, 'context_aware': 0.2}

        for name, model in models.items():
            try:
                result = model(data_array, current_context)
                ensemble_results[name] = result
                print(f"‚úÖ {name} model completed")
            except Exception as e:
                print(f"‚ùå {name} model failed: {str(e)}")
                ensemble_results[name] = {'forecast': 50.0, 'confidence': 0.1}  # Default fallback

        # Combine weighted predictions
        weighted_forecast = sum(
            ensemble_results[model]['forecast'] * weights[model]
            for model in ensemble_results
        )

        combined_confidence = sum(
            ensemble_results[model]['confidence'] * weights[model]
            for model in ensemble_results
        )

        ensemble_forecast = {
            'combined_prediction': weighted_forecast,
            'confidence': combined_confidence,
            'individual_models': ensemble_results
        }

        print(f"üéØ Ensemble prediction: {weighted_forecast:.1f}% (confidence: {combined_confidence:.2f})")
        return ensemble_forecast

    def seasonal_decomposition_model(self, data_array, current_context):
        """Decompose time series into seasonal, trend, and residual components"""
        try:
            if len(data_array) < 48:  # Need at least 2 days of 15-min intervals
                return {'forecast': 50.0, 'confidence': 0.2}

            occupancy = data_array['occupancy_ratio']
            timestamps = data_array['timestamp']

            # Extract hour of day for seasonal patterns
            hours = np.array([
                datetime.fromtimestamp(ts).hour for ts in timestamps
            ])

            # Calculate hourly averages for seasonal pattern
            hourly_patterns = {}
            for hour in range(24):
                hour_mask = hours == hour
                if np.any(hour_mask):
                    hourly_patterns[hour] = np.mean(occupancy[hour_mask])
                else:
                    hourly_patterns[hour] = 50.0  # Default

            # Get current hour for prediction
            current_hour = datetime.fromtimestamp(current_context.get('timestamp', time.time())).hour
            seasonal_forecast = hourly_patterns.get(current_hour, 50.0)

            # Calculate trend
            if len(occupancy) > 10:
                recent_trend = np.polyfit(range(len(occupancy[-10:])), occupancy[-10:], 1)[0]
                trend_adjusted_forecast = seasonal_forecast + (recent_trend * 4)  # Project 4 intervals ahead
            else:
                trend_adjusted_forecast = seasonal_forecast

            return {
                'forecast': max(0.0, min(100.0, trend_adjusted_forecast)),
                'confidence': min(0.8, len(data_array) / 100.0),
                'components': {
                    'seasonal': seasonal_forecast,
                    'trend': recent_trend if len(occupancy) > 10 else 0.0
                }
            }

        except Exception as e:
            print(f"‚ùå Seasonal decomposition error: {str(e)}")
            return {'forecast': 50.0, 'confidence': 0.1}

    def pattern_recognition_model(self, data_array, current_context):
        """Identify and predict based on recurring patterns"""
        try:
            if len(data_array) < 24:
                return {'forecast': 50.0, 'confidence': 0.2}

            occupancy = data_array['occupancy_ratio']
            timestamps = data_array['timestamp']

            # Identify day of week patterns
            weekdays = np.array([
                datetime.fromtimestamp(ts).weekday() for ts in timestamps
            ])

            current_weekday = datetime.fromtimestamp(current_context.get('timestamp', time.time())).weekday()
            current_hour = datetime.fromtimestamp(current_context.get('timestamp', time.time())).hour

            # Find similar time periods (same weekday and hour)
            similar_periods = []
            for i, (wd, ts) in enumerate(zip(weekdays, timestamps)):
                hour = datetime.fromtimestamp(ts).hour
                if wd == current_weekday and abs(hour - current_hour) <= 1:
                    similar_periods.append(occupancy[i])

            if similar_periods:
                pattern_forecast = np.mean(similar_periods)
                confidence = min(0.7, len(similar_periods) / 10.0)
            else:
                # Fallback to hourly pattern
                current_hour_mask = np.array([
                    datetime.fromtimestamp(ts).hour == current_hour for ts in timestamps
                ])
                if np.any(current_hour_mask):
                    pattern_forecast = np.mean(occupancy[current_hour_mask])
                    confidence = 0.3
                else:
                    pattern_forecast = np.mean(occupancy)
                    confidence = 0.1

            return {
                'forecast': max(0.0, min(100.0, pattern_forecast)),
                'confidence': confidence,
                'similar_periods_found': len(similar_periods)
            }

        except Exception as e:
            print(f"‚ùå Pattern recognition error: {str(e)}")
            return {'forecast': 50.0, 'confidence': 0.1}

    def trend_analysis_model(self, data_array, current_context):
        """Analyze short-term and long-term trends"""
        try:
            if len(data_array) < 10:
                return {'forecast': 50.0, 'confidence': 0.2}

            occupancy = data_array['occupancy_ratio']

            # Short-term trend (last 10 data points)
            short_term_x = np.arange(len(occupancy[-10:]))
            short_term_trend = np.polyfit(short_term_x, occupancy[-10:], 1)[0]

            # Long-term trend (all data)
            long_term_x = np.arange(len(occupancy))
            long_term_trend = np.polyfit(long_term_x, occupancy, 1)[0]

            # Current baseline
            current_baseline = np.mean(occupancy[-5:]) if len(occupancy) >= 5 else np.mean(occupancy)

            # Project trends forward (next hour = 4 intervals)
            short_term_projection = current_baseline + (short_term_trend * 4)
            long_term_projection = current_baseline + (long_term_trend * 4)

            # Weighted combination (favor short-term for immediate predictions)
            trend_forecast = short_term_projection * 0.7 + long_term_projection * 0.3

            # Confidence based on trend consistency
            trend_consistency = 1.0 - abs(short_term_trend - long_term_trend) / max(abs(short_term_trend), abs(long_term_trend), 1.0)
            confidence = min(0.6, trend_consistency * 0.6)

            return {
                'forecast': max(0.0, min(100.0, trend_forecast)),
                'confidence': confidence,
                'trends': {
                    'short_term': short_term_trend,
                    'long_term': long_term_trend,
                    'consistency': trend_consistency
                }
            }

        except Exception as e:
            print(f"‚ùå Trend analysis error: {str(e)}")
            return {'forecast': 50.0, 'confidence': 0.1}

    def context_aware_model(self, data_array, current_context):
        """Incorporate contextual factors like current status, recent patterns"""
        try:
            if len(data_array) < 5:
                return {'forecast': 50.0, 'confidence': 0.2}

            occupancy = data_array['occupancy_ratio']

            # Base prediction from recent average
            recent_average = np.mean(occupancy[-5:])

            # Context adjustments
            context_multiplier = 1.0

            # Current machine status influence
            current_status = current_context.get('status', 'unknown')
            if current_status == 'occupied':
                context_multiplier *= 1.3  # If currently occupied, likely to stay busier
            elif current_status == 'free':
                context_multiplier *= 0.8  # If currently free, might stay quieter

            # Time of day influence
            current_hour = datetime.fromtimestamp(current_context.get('timestamp', time.time())).hour
            if 6 <= current_hour <= 9 or 17 <= current_hour <= 21:  # Peak hours
                context_multiplier *= 1.2
            elif 22 <= current_hour <= 6:  # Off hours
                context_multiplier *= 0.6

            # Recent volatility
            if len(occupancy) >= 10:
                recent_volatility = np.std(occupancy[-10:])
                if recent_volatility > 20:  # High volatility
                    context_multiplier *= 1.1  # Expect continued activity

            context_forecast = recent_average * context_multiplier

            # Confidence based on data recency and consistency
            confidence = min(0.5, len(data_array) / 50.0)

            return {
                'forecast': max(0.0, min(100.0, context_forecast)),
                'confidence': confidence,
                'context_factors': {
                    'status_influence': current_status,
                    'time_influence': 'peak' if 6 <= current_hour <= 9 or 17 <= current_hour <= 21 else 'off-peak',
                    'volatility': recent_volatility if len(occupancy) >= 10 else 0
                }
            }

        except Exception as e:
            print(f"‚ùå Context-aware model error: {str(e)}")
            return {'forecast': 50.0, 'confidence': 0.1}

    def generate_gemini_insights(self, machine_id, data_array, forecast, anomalies):
        """Generate AI insights using Singapore Lambda for Gemini API calls with 30-minute caching"""
        import time

        # Create cache key based on machine_id and 30-minute time window
        current_time = int(time.time())
        cache_window = current_time // 1800  # 30 minutes = 1800 seconds
        cache_key = f"gemini_insights_{machine_id}_{cache_window}"

        # Check if we have cached insights for this machine in the current 30-min window
        try:
            # Try to get from in-memory cache (simple approach for MVP)
            if hasattr(self, '_insights_cache') and cache_key in self._insights_cache:
                cached_result = self._insights_cache[cache_key]
                print(f"üéØ Using cached Gemini insights for {machine_id} (saved API call)")
                return cached_result
        except Exception as e:
            print(f"‚ö†Ô∏è Cache check error: {str(e)}")

        # Initialize data_summary for fallback use
        data_summary = {
            'machine_id': machine_id,
            'total_data_points': len(data_array) if data_array is not None else 0,
            'date_range': 'unavailable',
            'avg_occupancy': 50,
            'peak_hours': 'unknown',
            'anomalies_count': len(anomalies) if anomalies else 0,
            'forecast_summary': forecast if forecast else {}
        }

        try:
            # Prepare detailed data summary for Singapore Lambda
            if data_array is not None and len(data_array) > 0:
                timestamps = data_array['timestamp']
                start_time = datetime.fromtimestamp(float(timestamps.min())).strftime('%Y-%m-%d')
                end_time = datetime.fromtimestamp(float(timestamps.max())).strftime('%Y-%m-%d')

                # Safely process forecast - handle ensemble_forecast structure
                forecast_summary = {}
                if forecast and isinstance(forecast, dict):
                    try:
                        # Extract key metrics from the ensemble forecast
                        combined_prediction = forecast.get('combined_prediction')
                        if hasattr(combined_prediction, 'item'):  # NumPy scalar
                            combined_prediction = float(combined_prediction.item())
                        elif isinstance(combined_prediction, (int, float)):
                            combined_prediction = float(combined_prediction)

                        confidence = forecast.get('confidence')
                        if hasattr(confidence, 'item'):  # NumPy scalar
                            confidence = float(confidence.item())
                        elif isinstance(confidence, (int, float)):
                            confidence = float(confidence)

                        forecast_summary = {
                            'predicted_usage': combined_prediction,
                            'confidence': confidence,
                            'model_count': len(forecast.get('individual_models', {}))
                        }
                    except Exception as fe:
                        print(f"‚ùå Error processing forecast: {str(fe)}")
                        forecast_summary = {}

                data_summary.update({
                    'date_range': f"{start_time} to {end_time}",
                    'avg_occupancy': round(float(np.mean(data_array['occupancy_ratio'])), 1),
                    'peak_hours': self.get_peak_hours_numpy(data_array),
                    'forecast_summary': forecast_summary
                })

            # Call Singapore Lambda for Gemini API access
            print(f"üá∏üá¨ Invoking Singapore Lambda for Gemini insights...")
            lambda_client = boto3.client('lambda', region_name='ap-southeast-1')

            payload = {
                'machine_id': machine_id,
                'data_summary': data_summary
            }

            response = lambda_client.invoke(
                FunctionName='gym-pulse-gemini-singapore',
                InvocationType='RequestResponse',
                Payload=json.dumps(payload)
            )

            # Process Singapore Lambda response
            result = json.loads(response['Payload'].read())

            if result.get('success'):
                ai_insights = result.get('insights', '')
                print(f"‚úÖ Received AI insights from Singapore: {len(ai_insights)} characters")

                # Cache the successful result for 30 minutes
                try:
                    if not hasattr(self, '_insights_cache'):
                        self._insights_cache = {}
                    self._insights_cache[cache_key] = ai_insights
                    print(f"üíæ Cached insights for {machine_id} (30min cache)")
                except Exception as cache_error:
                    print(f"‚ö†Ô∏è Cache storage error: {str(cache_error)}")

                return ai_insights
            else:
                print(f"‚ùå Singapore Lambda error: {result.get('error', 'Unknown error')}")
                # Try fallback insights from Singapore response
                fallback = result.get('fallback_insights')
                if fallback:
                    print(f"üîÑ Using Singapore fallback insights")
                    # Cache fallback insights too (shorter cache time)
                    try:
                        if not hasattr(self, '_insights_cache'):
                            self._insights_cache = {}
                        self._insights_cache[cache_key] = fallback
                        print(f"üíæ Cached fallback insights for {machine_id}")
                    except Exception as cache_error:
                        print(f"‚ö†Ô∏è Cache storage error: {str(cache_error)}")
                    return fallback
                else:
                    local_fallback = self.fallback_insights(machine_id, data_summary)
                    # Cache local fallback too
                    try:
                        if not hasattr(self, '_insights_cache'):
                            self._insights_cache = {}
                        self._insights_cache[cache_key] = local_fallback
                        print(f"üíæ Cached local fallback for {machine_id}")
                    except Exception as cache_error:
                        print(f"‚ö†Ô∏è Cache storage error: {str(cache_error)}")
                    return local_fallback

        except Exception as e:
            print(f"‚ùå Error calling Singapore Lambda: {str(e)}")
            local_fallback = self.fallback_insights(machine_id, data_summary)
            # Cache fallback insights to avoid repeated API calls on failures
            try:
                if not hasattr(self, '_insights_cache'):
                    self._insights_cache = {}
                self._insights_cache[cache_key] = local_fallback
                print(f"üíæ Cached error fallback for {machine_id}")
            except Exception as cache_error:
                print(f"‚ö†Ô∏è Cache storage error: {str(cache_error)}")
            return local_fallback

    def fallback_insights(self, machine_id, data_summary):
        """Provide basic insights when Gemini API is unavailable"""
        avg_occupancy = data_summary.get('avg_occupancy', 50)

        if avg_occupancy > 70:
            usage_level = "high usage patterns"
            recommendation = "Consider visiting during off-peak hours for better availability"
        elif avg_occupancy > 40:
            usage_level = "moderate usage patterns"
            recommendation = "Generally good availability with some busy periods"
        else:
            usage_level = "low usage patterns"
            recommendation = "Excellent availability throughout the day"

        return f"Analysis for {machine_id}: {usage_level} detected over {data_summary['total_data_points']} data points. {recommendation}. Peak activity during {data_summary.get('peak_hours', 'various')} hours."

    def get_peak_hours_numpy(self, data_array):
        """Identify peak usage hours using NumPy operations"""
        try:
            # Handle both structured arrays and regular arrays
            if hasattr(data_array, 'dtype') and data_array.dtype.names:
                # Structured array - access fields correctly
                timestamps = data_array['timestamp']
                occupancy = data_array['occupancy_ratio']
            elif isinstance(data_array, np.ndarray) and len(data_array) > 0:
                # Regular array - assume first column is timestamp, second is occupancy
                timestamps = data_array[:, 0] if data_array.ndim > 1 else data_array
                occupancy = data_array[:, 1] if data_array.ndim > 1 else np.ones_like(data_array) * 50
            else:
                # Empty or invalid data
                return "No data"

            # Group by hour
            hourly_avg = {}
            for ts, occ in zip(timestamps, occupancy):
                try:
                    hour = datetime.fromtimestamp(float(ts)).hour
                    if hour not in hourly_avg:
                        hourly_avg[hour] = []
                    hourly_avg[hour].append(float(occ))
                except (ValueError, TypeError, OSError):
                    continue

            # Calculate averages and find peaks
            if not hourly_avg:
                return "No valid data"

            hour_averages = {hour: np.mean(values) for hour, values in hourly_avg.items()}

            # Find hours with above-average usage
            overall_avg = np.mean(list(hour_averages.values()))
            peak_hours = [hour for hour, avg in hour_averages.items() if avg > overall_avg]

            if peak_hours:
                return f"{min(peak_hours)}-{max(peak_hours)}"
            else:
                return "No clear peaks"

        except Exception as e:
            print(f"‚ùå Error calculating peak hours: {str(e)}")
            return "Analysis pending"

    def generate_forecast_with_confidence(self, ensemble_forecast, current_context):
        """Generate final forecast with confidence intervals"""
        try:
            prediction = ensemble_forecast['combined_prediction']
            confidence = ensemble_forecast['confidence']

            # Classification based on prediction
            if prediction >= 70:
                classification = "likely_occupied"
                display_text = "AI: Busy period"
                color = "red"
            elif prediction >= 40:
                classification = "possibly_occupied"
                display_text = "AI: Moderate usage"
                color = "orange"
            else:
                classification = "likely_free"
                display_text = "AI: Available soon"
                color = "green"

            # Confidence score (0-100)
            confidence_score = int(confidence * 100)

            return {
                'likelyFreeIn30m': prediction < 40,
                'classification': classification,
                'display_text': display_text,
                'color': color,
                'confidence': confidence_score,
                'show_to_user': confidence_score > 20,
                'forecast_usage': round(prediction, 1),
                'based_on_ai': True
            }

        except Exception as e:
            print(f"‚ùå Error generating forecast: {str(e)}")
            return {
                'likelyFreeIn30m': False,
                'classification': 'unknown',
                'display_text': 'AI: Analyzing...',
                'color': 'gray',
                'confidence': 0,
                'show_to_user': False,
                'forecast_usage': 50.0,
                'based_on_ai': False
            }

    def calculate_confidence_score(self, data_array, anomalies):
        """Calculate overall confidence in predictions"""
        try:
            base_confidence = min(len(data_array) / 100.0, 0.8)  # More data = higher confidence

            # Reduce confidence if many anomalies
            anomaly_penalty = len(anomalies) / max(len(data_array), 1) * 0.3

            final_confidence = max(0.1, base_confidence - anomaly_penalty)
            return round(final_confidence, 2)

        except Exception:
            return 0.3

    def fallback_forecast(self, machine_id, current_context):
        """Simple fallback when ML models fail"""
        return {
            'forecast': {
                'likelyFreeIn30m': False,
                'classification': 'insufficient_data',
                'display_text': 'AI: Learning patterns...',
                'color': 'gray',
                'confidence': 10,
                'show_to_user': True,
                'forecast_usage': 50.0,
                'based_on_ai': True
            },
            'anomalies': [],
            'ml_insights': f"Basic forecast for {machine_id} - building ML models with incoming data",
            'confidence_score': 0.1,
            'models_used': 0,
            'data_points_analyzed': 0
        }


class GeminiChatEngine:
    """
    Gemini-powered conversational chat engine for gym equipment queries
    Replaces Bedrock with Gemini for unified AI architecture
    """

    def __init__(self):
        self.dynamodb = dynamodb
        self.current_state_table = current_state_table

    def process_chat_request(self, user_message, user_location, session_id):
        """
        Process chat request using Gemini for natural language understanding and tool orchestration
        """
        try:
            print(f"üß† Processing chat with Gemini: '{user_message}'")

            # Step 1: Detect category from user message
            category = self.detect_category_from_message(user_message.lower())

            # Step 2: If location and category available, get availability data
            if user_location and category:
                # Get availability data
                availability_data = self.get_availability_by_category(
                    user_location['lat'], user_location['lon'], category, radius=10
                )

                # Get route matrix if we have branches
                branches = availability_data.get('branches', [])
                available_branches = [b for b in branches if b.get('freeCount', 0) > 0]

                if available_branches:
                    route_data = self.get_route_matrix(user_location, available_branches)
                    # Generate Gemini response with data
                    response_text = self.generate_gemini_chat_response(
                        user_message, category, availability_data, route_data
                    )
                    tools_used = ['getAvailabilityByCategory', 'getRouteMatrix']
                else:
                    # No available machines
                    response_text = self.generate_no_availability_response(category, branches)
                    tools_used = ['getAvailabilityByCategory']

            elif not user_location:
                # Need location
                response_text = "I'd love to help you find available gym equipment! However, I need your location to provide personalized recommendations. Please share your location and I'll find the best options nearby."
                tools_used = []

            elif not category:
                # Need category clarification
                response_text = "I can help you find available gym equipment! Which type of workout are you interested in: legs, chest, or back exercises?"
                tools_used = []

            else:
                # Generic response
                response_text = self.generate_gemini_generic_response(user_message)
                tools_used = []

            return {
                'response': response_text,
                'sessionId': session_id,
                'toolsUsed': tools_used,
                'timestamp': datetime.utcnow().isoformat(),
                'gemini_powered': True
            }

        except Exception as e:
            print(f"‚ùå Error in Gemini chat processing: {str(e)}")
            return {
                'response': "I'm experiencing some technical difficulties right now. Please try again in a moment.",
                'sessionId': session_id,
                'toolsUsed': [],
                'error': str(e)
            }

    def detect_category_from_message(self, message):
        """Simple keyword detection for workout categories"""
        legs_keywords = ['leg', 'legs', 'squat', 'quad', 'calf', 'thigh', 'lower body']
        chest_keywords = ['chest', 'bench', 'press', 'pecs', 'upper body']
        back_keywords = ['back', 'lat', 'pull', 'row', 'pulldown', 'pullup']

        message_lower = message.lower()

        if any(keyword in message_lower for keyword in legs_keywords):
            return 'legs'
        elif any(keyword in message_lower for keyword in chest_keywords):
            return 'chest'
        elif any(keyword in message_lower for keyword in back_keywords):
            return 'back'

        return None

    def get_availability_by_category(self, lat, lon, category, radius=10):
        """Get machine availability for category near user location"""
        try:
            # Get all machines for the category
            response = current_state_table.scan(
                FilterExpression='category = :cat',
                ExpressionAttributeValues={':cat': category}
            )

            machines = response.get('Items', [])

            # Group by branch and calculate availability
            branches = {}
            for machine in machines:
                gym_id = machine.get('gymId')
                if gym_id not in branches:
                    coords = get_branch_coordinates(gym_id)
                    # Calculate distance
                    distance = self.calculate_distance(lat, lon, coords['lat'], coords['lon'])
                    if distance <= radius:
                        branches[gym_id] = {
                            'branchId': gym_id,
                            'name': gym_id.replace('-', ' ').title(),
                            'lat': coords['lat'],
                            'lon': coords['lon'],
                            'freeCount': 0,
                            'totalCount': 0,
                            'distance': distance
                        }

                if gym_id in branches:
                    branches[gym_id]['totalCount'] += 1
                    if machine.get('status') == 'free':
                        branches[gym_id]['freeCount'] += 1

            return {
                'branches': list(branches.values()),
                'category': category,
                'searchRadius': radius
            }

        except Exception as e:
            print(f"‚ùå Error getting availability: {str(e)}")
            return {'branches': [], 'category': category, 'searchRadius': radius}

    def get_route_matrix(self, user_location, branches):
        """Calculate travel times using Google Maps API for accurate walking times"""
        try:
            # Try to get real walking times from Google Maps API
            google_api_key = os.environ.get('GOOGLE_MAPS_API_KEY')

            if google_api_key and len(branches) <= 5:  # Rate limit protection
                return self.get_google_walking_times(user_location, branches, google_api_key)
            else:
                # Fallback to improved estimation
                return self.get_estimated_walking_times(branches)

        except Exception as e:
            print(f"‚ùå Error calculating routes: {str(e)}")
            # Always fallback to estimation
            return self.get_estimated_walking_times(branches)

    def get_google_walking_times(self, user_location, branches, api_key):
        """Get actual walking and transit times from Google Maps API"""
        try:
            import requests

            print(f"üó∫Ô∏è Using Google Maps API for {len(branches)} destinations")

            # Build destinations string
            destinations = []
            for branch in branches:
                destinations.append(f"{branch['lat']},{branch['lon']}")

            routes = []

            # Get walking times first
            walking_params = {
                'origins': f"{user_location['lat']},{user_location['lon']}",
                'destinations': '|'.join(destinations),
                'mode': 'walking',
                'units': 'metric',
                'key': api_key
            }

            walking_response = requests.get("https://maps.googleapis.com/maps/api/distancematrix/json",
                                          params=walking_params, timeout=5)
            walking_data = walking_response.json()
            print(f"üö∂ Google Maps walking API response: {walking_data.get('status')}")

            # Get transit times for comparison
            transit_params = {
                'origins': f"{user_location['lat']},{user_location['lon']}",
                'destinations': '|'.join(destinations),
                'mode': 'transit',
                'units': 'metric',
                'key': api_key
            }

            transit_response = requests.get("https://maps.googleapis.com/maps/api/distancematrix/json",
                                          params=transit_params, timeout=5)
            transit_data = transit_response.json()
            print(f"üöá Google Maps transit API response: {transit_data.get('status')}")

            if walking_data.get('status') == 'OK':
                walking_elements = walking_data['rows'][0]['elements']
                transit_elements = transit_data['rows'][0]['elements'] if transit_data.get('status') == 'OK' else []

                for i, walking_element in enumerate(walking_elements):
                    if walking_element['status'] == 'OK':
                        walking_duration = walking_element['duration']['value']
                        walking_distance = walking_element['distance']['value']
                        walking_minutes = max(1, round(walking_duration / 60))

                        # Check if transit is available and faster
                        transit_info = None
                        if i < len(transit_elements) and transit_elements[i]['status'] == 'OK':
                            transit_duration = transit_elements[i]['duration']['value']
                            transit_minutes = max(1, round(transit_duration / 60))

                            if transit_minutes < walking_minutes and walking_minutes > 15:
                                transit_info = {
                                    'minutes': transit_minutes,
                                    'method': 'transit'
                                }

                        # Determine best transportation method
                        if walking_minutes <= 15:  # 15 min or less, just walk
                            transport_method = f"üö∂ {walking_minutes} min walk"
                            final_minutes = walking_minutes
                        elif transit_info and transit_info['minutes'] < walking_minutes - 5:  # Transit saves 5+ min
                            transport_method = f"üöá {transit_info['minutes']} min (transit)"
                            final_minutes = transit_info['minutes']
                        else:  # Walking is still reasonable or transit not much faster
                            if walking_minutes <= 25:
                                transport_method = f"üö∂ {walking_minutes} min walk"
                            else:
                                transport_method = f"üö∂ {walking_minutes} min walk (or transit)"
                            final_minutes = walking_minutes

                        routes.append({
                            'branchId': branches[i]['branchId'],
                            'etaMinutes': final_minutes,
                            'distanceKm': round(walking_distance / 1000, 1),
                            'transportMethod': transport_method,
                            'source': 'google_maps'
                        })

                        print(f"üìç {branches[i]['branchId']}: {transport_method}")

                    else:
                        # Fallback for this specific branch
                        distance_km = branches[i].get('distance', 0)
                        eta_minutes = max(1, round((distance_km / 5) * 60))
                        routes.append({
                            'branchId': branches[i]['branchId'],
                            'etaMinutes': eta_minutes,
                            'distanceKm': round(distance_km, 1),
                            'transportMethod': f"üö∂ ~{eta_minutes} min walk",
                            'source': 'estimated'
                        })

                return {'routes': routes}
            else:
                raise Exception(f"Google Maps API error: {walking_data.get('status')}")

        except Exception as e:
            print(f"‚ùå Google Maps API failed: {str(e)}, falling back to estimation")
            return self.get_estimated_walking_times(branches)

    def get_estimated_walking_times(self, branches):
        """Fallback to improved walking time estimation"""
        routes = []
        for branch in branches:
            distance_km = branch.get('distance', 0)

            # More realistic Hong Kong walking time estimation
            if distance_km <= 1.0:
                # Walking for short distances (5 km/h)
                eta_minutes = max(1, round((distance_km / 5) * 60))
            elif distance_km <= 3.0:
                # Mixed walking + MTR for medium distances (12 km/h average)
                eta_minutes = max(3, round((distance_km / 12) * 60))
            else:
                # MTR/transport for longer distances (20 km/h average with transfers)
                eta_minutes = max(5, round((distance_km / 20) * 60))

            routes.append({
                'branchId': branch['branchId'],
                'etaMinutes': eta_minutes,
                'distanceKm': round(distance_km, 1),
                'source': 'estimated'
            })

        return {'routes': routes}

    def calculate_distance(self, lat1, lon1, lat2, lon2):
        """Calculate distance between two points using Haversine formula"""
        import math

        R = 6371  # Earth's radius in kilometers

        lat1_rad = math.radians(lat1)
        lat2_rad = math.radians(lat2)
        delta_lat = math.radians(lat2 - lat1)
        delta_lon = math.radians(lon2 - lon1)

        a = (math.sin(delta_lat / 2) ** 2 +
             math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2) ** 2)
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

        return R * c

    def generate_gemini_chat_response(self, user_message, category, availability_data, route_data):
        """Generate conversational response using Gemini API"""
        try:
            # Prepare data for Gemini
            branches = availability_data.get('branches', [])
            routes_by_branch = {route['branchId']: route for route in route_data.get('routes', [])}

            # Sort branches by ETA then by availability
            sorted_branches = sorted(branches,
                key=lambda b: (routes_by_branch.get(b['branchId'], {}).get('etaMinutes', 999), -b.get('freeCount', 0))
            )

            # Build context for Gemini
            context = {
                'user_query': user_message,
                'category': category,
                'branches_found': len(branches),
                'available_branches': [b for b in branches if b.get('freeCount', 0) > 0]
            }

            # Create detailed response
            if sorted_branches and sorted_branches[0].get('freeCount', 0) > 0:
                top_branch = sorted_branches[0]
                route_info = routes_by_branch.get(top_branch['branchId'], {})

                response = f"Great! I found {category} equipment available nearby:\n\n"
                response += f"ü•á **{top_branch['name']}** (Recommended)\n"
                response += f"‚Ä¢ {top_branch['freeCount']}/{top_branch['totalCount']} {category} machines available\n"

                if route_info.get('etaMinutes'):
                    response += f"‚Ä¢ {route_info['etaMinutes']} minutes away\n"

                # Add alternatives
                alternatives = sorted_branches[1:3]
                if alternatives:
                    response += f"\n**Other options:**\n"
                    for branch in alternatives:
                        if branch.get('freeCount', 0) > 0:
                            alt_route = routes_by_branch.get(branch['branchId'], {})
                            response += f"‚Ä¢ {branch['name']}: {branch['freeCount']} available"
                            if alt_route.get('etaMinutes'):
                                response += f" ({alt_route['etaMinutes']} min)"
                            response += "\n"

                response += "\nHave a great workout! üí™"

            else:
                response = self.generate_no_availability_response(category, branches)

            return response

        except Exception as e:
            print(f"‚ùå Error generating Gemini response: {str(e)}")
            return f"I found some {category} equipment information, but I'm having trouble formatting the response. Please try again."

    def generate_no_availability_response(self, category, branches):
        """Generate response when no machines are available"""
        if branches:
            response = f"All {category} machines are currently occupied at nearby gyms. Here's what I found:\n\n"
            for branch in branches[:2]:
                response += f"‚Ä¢ {branch.get('name', branch['branchId'])}: {branch.get('totalCount', 0)} {category} machines (all occupied)\n"
            response += f"\nI recommend checking back in 15-30 minutes when machines might become available!"
        else:
            response = f"I couldn't find any {category} equipment nearby. You might want to try expanding your search radius or check back later."

        return response

    def generate_gemini_generic_response(self, user_message):
        """Generate generic response for non-specific queries"""
        return "I'm your gym equipment assistant! I can help you find available machines for legs, chest, or back workouts. Just tell me what you'd like to work on and share your location, and I'll find the best options nearby."


def lambda_handler(event, context):
    """
    AWS Lambda handler for GymPulse API requests with ML forecasting
    """

    # CORS headers for all responses
    cors_headers = {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token',
        'Access-Control-Allow-Methods': 'GET,POST,PUT,DELETE,OPTIONS'
    }

    try:
        # Handle preflight CORS requests
        if event.get('httpMethod') == 'OPTIONS':
            return {
                'statusCode': 200,
                'headers': cors_headers,
                'body': json.dumps({'message': 'CORS preflight'})
            }

        # Extract HTTP method and path
        http_method = event.get('httpMethod', 'GET')
        path = event.get('path', '')

        print(f"Processing {http_method} {path}")

        # Route to appropriate handler - order matters!
        # Check specific routes first, then general ones
        if path.startswith('/branches/') and '/peak-hours' in path and http_method == 'GET':
            return handle_peak_hours_request(event, context, cors_headers)
        elif path.startswith('/branches') and http_method == 'GET':
            if path.endswith('/branches'):
                return handle_branches_request(event, context, cors_headers)
            elif '/categories/' in path and '/machines' in path:
                return handle_machines_request(event, context, cors_headers)
            else:
                return {
                    'statusCode': 404,
                    'headers': {'Content-Type': 'application/json', **cors_headers},
                    'body': json.dumps({'error': 'Branch endpoint not found'})
                }
        elif path.startswith('/machines/') and '/history' in path and http_method == 'GET':
            return handle_machine_history_request(event, context, cors_headers)
        elif path.startswith('/alerts') and http_method == 'POST':
            return handle_alerts_request(event, context, cors_headers)
        elif path.startswith('/chat') and http_method == 'POST':
            return handle_chat_request(event, context, cors_headers)
        else:
            return {
                'statusCode': 404,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'error': f'Endpoint not found: {http_method} {path}'})
            }

    except Exception as e:
        print(f"‚ùå Lambda handler error: {str(e)}")
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'error': 'Internal server error', 'details': str(e)})
        }


def handle_branches_request(event, context, cors_headers):
    """
    Handle GET /branches - return list of branches with availability counts
    """
    try:
        print("Fetching all branches with availability data")

        # Get all current machine states
        response = current_state_table.scan()
        machines = response.get('Items', [])

        # Group by branch
        branches = {}
        for machine in machines:
            gym_id = machine.get('gymId')
            category = machine.get('category')
            status = machine.get('status', 'unknown')

            if gym_id not in branches:
                branches[gym_id] = {
                    'id': gym_id,
                    'name': gym_id.replace('-', ' ').title() + ' Branch',
                    'coordinates': get_branch_coordinates(gym_id),
                    'categories': {}
                }

            if category not in branches[gym_id]['categories']:
                branches[gym_id]['categories'][category] = {'free': 0, 'total': 0}

            branches[gym_id]['categories'][category]['total'] += 1
            if status == 'free':
                branches[gym_id]['categories'][category]['free'] += 1

        # Convert to list format
        branches_list = list(branches.values())

        print(f"Returning {len(branches_list)} branches")

        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'branches': branches_list})
        }

    except Exception as e:
        print(f"‚ùå Error in branches request: {str(e)}")
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'error': 'Failed to fetch branches'})
        }


def handle_machines_request(event, context, cors_headers):
    """
    Handle GET /branches/{branchId}/categories/{category}/machines
    """
    try:
        # Extract branch ID and category from path
        path = event.get('path', '')
        # Path format: /branches/{branchId}/categories/{category}/machines
        path_parts = path.split('/')
        if len(path_parts) >= 6:
            branch_id = path_parts[2]
            category = path_parts[4]
        else:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'error': 'Invalid path format'})
            }

        print(f"Fetching machines for branch: {branch_id}, category: {category}")

        # Query machines for this branch and category
        response = current_state_table.scan(
            FilterExpression='gymId = :gym_id AND category = :cat',
            ExpressionAttributeValues={
                ':gym_id': branch_id,
                ':cat': category
            }
        )

        machines = response.get('Items', [])

        # Format machine data with human-readable names
        machine_list = []
        for machine in machines:
            machine_id = machine.get('machineId')

            machine_data = {
                'machineId': machine_id,
                'name': format_machine_name(machine_id),
                'status': machine.get('status', 'unknown'),
                'lastUpdate': float(machine.get('lastUpdate', 0)),
                'category': machine.get('category'),
                'gymId': machine.get('gymId'),
                'type': get_machine_type(machine_id),
                'alertEligible': machine.get('status') == 'occupied'  # Can set alert if occupied
            }
            machine_list.append(machine_data)

        print(f"Returning {len(machine_list)} machines")

        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'machines': machine_list})
        }

    except Exception as e:
        print(f"‚ùå Error in machines request: {str(e)}")
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'error': 'Failed to fetch machines'})
        }


def handle_machine_history_request(event, context, cors_headers):
    """
    Handle GET /machines/{machineId}/history - return usage history for heatmap with ML forecasting
    """
    import time
    from datetime import datetime, timedelta

    try:
        # Extract machine ID from path
        path = event.get('path', '')
        # Path format: /machines/{machineId}/history
        path_parts = path.split('/')
        if len(path_parts) >= 3:
            machine_id = path_parts[2]
        else:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'error': 'Missing machine ID'})
            }

        # Get query parameters
        query_params = event.get('queryStringParameters') or {}
        range_param = query_params.get('range', '24h')

        print(f"Fetching history for machine: {machine_id}, range: {range_param}")

        # Get machine info from current state to find category and gymId
        machine_response = current_state_table.get_item(Key={'machineId': machine_id})
        if 'Item' not in machine_response:
            print(f"Machine {machine_id} not found in current state")
            return {
                'statusCode': 200,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'usageData': [], 'machineId': machine_id})
            }

        machine = machine_response['Item']
        gym_id = machine.get('gymId')
        category = machine.get('category')

        if not gym_id or not category:
            print(f"Missing gymId or category for machine {machine_id}")
            return {
                'statusCode': 200,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'usageData': [], 'machineId': machine_id})
            }

        # Calculate time range - expand to include existing historical data
        current_time = int(time.time())
        start_time = current_time - (20 * 24 * 60 * 60)  # 20 days ago to include existing data

        # Query aggregates table for this gym/category combination
        gym_category_key = f"{gym_id}_{category}"

        response = aggregates_table.query(
            KeyConditionExpression='gymId_category = :gck AND timestamp15min BETWEEN :start AND :end',
            ExpressionAttributeValues={
                ':gck': gym_category_key,
                ':start': start_time,
                ':end': current_time
            },
            ScanIndexForward=True  # Sort by timestamp ascending
        )

        aggregates = response.get('Items', [])
        print(f"Found {len(aggregates)} aggregate records for {gym_category_key}")

        # Initialize ML forecasting engine
        ml_engine = MLForecastEngine()

        # Generate ML-powered forecast
        current_context = {
            'machine_id': machine_id,
            'status': machine.get('status', 'unknown'),
            'timestamp': current_time,
            'category': category,
            'gym_id': gym_id
        }

        forecast_result = ml_engine.generate_ai_forecast(machine_id, aggregates, current_context)

        # Convert aggregates to hourly usage data for heatmap
        usage_data = []

        # Group aggregates by hour
        hourly_data = {}
        for aggregate in aggregates:
            timestamp = int(aggregate['timestamp15min'])
            # Convert UTC timestamp to Hong Kong time
            utc_time = datetime.fromtimestamp(timestamp)
            hk_aggregate_time = utc_time + timedelta(hours=8)
            hour = hk_aggregate_time.hour
            occupancy_ratio = float(aggregate.get('occupancyRatio', 0))
            # Fix potential occupancy ratio scaling issues
            if occupancy_ratio > 100:
                occupancy_ratio = occupancy_ratio / 10  # Convert 960 -> 96.0
            if occupancy_ratio > 100:
                occupancy_ratio = 100  # Cap at 100%

            if hour not in hourly_data:
                hourly_data[hour] = []
            hourly_data[hour].append(occupancy_ratio)

        # Generate real forecast based on historical data only
        # Use Hong Kong timezone (UTC+8)
        utc_now = datetime.utcnow()
        hk_time = utc_now + timedelta(hours=8)  # Hong Kong is UTC+8
        current_hour = hk_time.hour

        # Only generate forecast if we have sufficient historical data
        if len(hourly_data) < 12:  # Need at least 12 hours of historical data for meaningful forecast
            usage_data = []
        else:
            # Real forecasting system: hourly updated predictions for today
            for hour in range(24):
                if hour < current_hour:
                    # Past hours: use actual historical data if available
                    if hour in hourly_data:
                        avg_usage_percentage = sum(hourly_data[hour]) / len(hourly_data[hour])
                    else:
                        continue  # Skip hours without historical data
                    data_type = 'historical'
                elif hour == current_hour:
                    # Current hour: blend historical pattern with real-time adjustment
                    if hour in hourly_data:
                        historical_avg = sum(hourly_data[hour]) / len(hourly_data[hour])
                        # Adjust based on current machine status (simple real-time correction)
                        current_status = machine.get('status', 'unknown')
                        if current_status == 'occupied':
                            avg_usage_percentage = min(95.0, historical_avg * 1.2)  # Increase if currently occupied
                        else:
                            avg_usage_percentage = max(5.0, historical_avg * 0.8)   # Decrease if currently free
                    else:
                        continue  # Skip if no historical data for current hour
                    data_type = 'current'
                else:
                    # Future hours: forecast based on historical patterns and ML predictions
                    if hour in hourly_data:
                        # Base forecast on historical average
                        historical_avg = sum(hourly_data[hour]) / len(hourly_data[hour])

                        # Apply ML adjustment if forecast is available and confident
                        if (forecast_result and
                            forecast_result.get('forecast', {}).get('confidence', 0) > 20):
                            ml_prediction = forecast_result['forecast'].get('forecast_usage', historical_avg)
                            # Blend historical and ML prediction (weighted by ML confidence)
                            ml_weight = forecast_result['forecast'].get('confidence', 0) / 100.0
                            avg_usage_percentage = historical_avg * (1 - ml_weight) + ml_prediction * ml_weight
                        else:
                            avg_usage_percentage = historical_avg
                    else:
                        continue  # Skip future hours without historical baseline
                    data_type = 'forecast'

                usage_data.append({
                    'hour': hour,
                    'day_of_week': hk_time.weekday(),
                    'usage_percentage': round(avg_usage_percentage, 1),
                    'timestamp': hk_time.isoformat(),
                    'predicted_free_time': int((100 - avg_usage_percentage) * 60 / 100) if avg_usage_percentage < 100 else 0,
                    'data_type': data_type,  # 'historical', 'current', or 'forecast'
                    'confidence': 'high' if data_type == 'historical' else 'medium' if data_type == 'current' else 'low'
                })

        # Include current status information from the machine data we already fetched
        current_status = {
            'status': machine.get('status', 'unknown'),
            'lastUpdate': int(machine.get('lastUpdate', 0)),
            'gymId': gym_id,
            'category': category,
            'name': machine_id,
            'alertEligible': machine.get('status') == 'occupied'
        }

        # Build response with ML forecast
        response_data = {
            'usageData': usage_data,
            'machineId': machine_id,
            'gymId': gym_id,
            'category': category,
            'currentStatus': current_status,
            'dataPoints': len(aggregates),
            'timeRange': f"24h ({len(aggregates)} 15-min intervals)"
        }

        # Add ML forecast results
        if forecast_result:
            forecast_data = forecast_result.get('forecast', {})
            response_data['forecast'] = forecast_data

            # Extract ml_insights from nested forecast structure
            response_data['ml_insights'] = forecast_data.get('ml_insights', 'ML analysis in progress')
            response_data['anomalies'] = forecast_result.get('anomalies', [])

            # Also ensure anomalies_detected is correct
            if response_data['anomalies']:
                response_data['forecast']['anomalies_detected'] = len(response_data['anomalies'])
            else:
                response_data['forecast']['anomalies_detected'] = forecast_data.get('anomalies_detected', 0)

            response_data['forecast']['models_used'] = forecast_result.get('models_used', 0)
        else:
            # Fallback forecast
            response_data['forecast'] = {
                'likelyFreeIn30m': False,
                'classification': 'insufficient_data',
                'display_text': 'AI: Analyzing patterns...',
                'color': 'gray',
                'confidence': 10,
                'show_to_user': True,
                'forecast_usage': 50.0,
                'based_on_ai': True,
                'models_used': 0,
                'anomalies_detected': 0
            }
            response_data['ml_insights'] = f'Building ML models for {machine_id} - analyzing {len(aggregates)} data points'
            response_data['anomalies'] = []

        print(f"Returning {len(usage_data)} usage data points with ML forecast")

        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps(response_data, default=str)
        }

    except Exception as e:
        print(f"‚ùå Error in machine history request: {str(e)}")
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'error': 'Failed to fetch machine history', 'details': str(e)})
        }


def handle_alerts_request(event, context, cors_headers):
    """
    Handle POST /alerts - create alert subscription
    """
    try:
        # Parse request body
        body = json.loads(event.get('body', '{}'))
        machine_id = body.get('machineId')
        user_id = body.get('userId', 'anonymous')
        quiet_hours = body.get('quietHours', {'start': 22, 'end': 7})

        if not machine_id:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'error': 'Missing machineId'})
            }

        print(f"Creating alert for machine: {machine_id}, user: {user_id}")

        # TODO: Implement alert creation logic
        # For now, return a success response
        alert_data = {
            'alertId': f"alert_{machine_id}_{user_id}_{int(time.time())}",
            'machineId': machine_id,
            'userId': user_id,
            'quietHours': quiet_hours,
            'active': True,
            'createdAt': datetime.utcnow().isoformat()
        }

        return {
            'statusCode': 201,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'alert': alert_data})
        }

    except Exception as e:
        print(f"‚ùå Error in alerts request: {str(e)}")
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'error': 'Failed to create alert'})
        }


def handle_chat_request(event, context, cors_headers):
    """
    Handle POST /chat - Gemini-powered chatbot with tool-use capabilities
    """
    try:
        # Handle OPTIONS method for CORS preflight
        if event.get('httpMethod') == 'OPTIONS':
            return {
                'statusCode': 200,
                'headers': cors_headers,
                'body': json.dumps({'message': 'CORS preflight for chat endpoint'})
            }

        # Parse request body for POST requests
        body = json.loads(event.get('body', '{}'))
        user_message = body.get('message', '')
        user_location = body.get('userLocation')  # {lat, lon}
        session_id = body.get('sessionId', 'default')

        if not user_message:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'error': 'Missing message'})
            }

        print(f"ü§ñ Gemini Chat: Processing '{user_message[:50]}...' with location: {user_location}")

        # Initialize Gemini chat engine
        chat_engine = GeminiChatEngine()

        # Process the chat request with Gemini
        chat_response = chat_engine.process_chat_request(user_message, user_location, session_id)

        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps(chat_response)
        }

    except Exception as e:
        print(f"‚ùå Error in Gemini chat request: {str(e)}")
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'error': 'Failed to process chat request', 'details': str(e)})
        }


def handle_peak_hours_request(event, context, cors_headers):
    """
    Handle GET /branches/{branchId}/peak-hours - return ML-based peak hours forecast for a branch
    """
    try:
        # Extract branch ID from path: /branches/{branchId}/peak-hours
        path = event.get('path', '')
        path_parts = path.split('/')
        if len(path_parts) >= 3:
            branch_id = path_parts[2]
        else:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'error': 'Invalid path format. Expected: /branches/{branchId}/peak-hours'})
            }

        print(f"ü§ñ Generating ML-based peak hours forecast for branch: {branch_id}")

        # Initialize ML Forecast Engine
        ml_engine = MLForecastEngine()

        # Get all machines for this branch
        response = current_state_table.scan(
            FilterExpression='gymId = :gym_id',
            ExpressionAttributeValues={':gym_id': branch_id}
        )
        machines = response.get('Items', [])

        if not machines:
            return {
                'statusCode': 404,
                'headers': {'Content-Type': 'application/json', **cors_headers},
                'body': json.dumps({'error': f'No machines found for branch: {branch_id}'})
            }

        # Calculate current occupancy
        total_machines = len(machines)
        occupied_machines = sum(1 for m in machines if m.get('status') == 'occupied')
        current_occupancy_rate = (occupied_machines / total_machines) * 100

        # Get current time in Hong Kong timezone (UTC+8)
        utc_now = datetime.utcnow()
        hk_time = utc_now + timedelta(hours=8)
        current_hour = hk_time.hour

        print(f"üìä Branch {branch_id}: {occupied_machines}/{total_machines} occupied ({current_occupancy_rate:.1f}%)")

        # Get historical aggregates data for ML forecasting
        try:
            # Query last 7 days of aggregated data for this branch
            seven_days_ago = int((utc_now - timedelta(days=7)).timestamp())

            historical_response = aggregates_table.scan(
                FilterExpression='begins_with(gymId_category, :branch_prefix) AND timestamp15min >= :start_time',
                ExpressionAttributeValues={
                    ':branch_prefix': f"{branch_id}_",
                    ':start_time': seven_days_ago
                },
                Limit=1000  # Limit to prevent timeout
            )

            historical_data = historical_response.get('Items', [])
            print(f"üìà Retrieved {len(historical_data)} historical data points for ML analysis")

        except Exception as e:
            print(f"‚ö†Ô∏è  Could not retrieve historical data: {str(e)}")
            historical_data = []

        # Generate ML-based forecast if we have sufficient historical data
        if len(historical_data) >= 50:
            print(f"üß† Using ML forecasting with {len(historical_data)} data points")

            # Use the first machine as representative for branch-level analysis
            representative_machine = machines[0]['machineId']

            # Generate AI forecast using ML engine
            current_context = {
                'current_hour': current_hour,
                'current_occupancy': current_occupancy_rate,
                'branch_id': branch_id,
                'total_machines': total_machines
            }

            ml_forecast = ml_engine.generate_ai_forecast(
                representative_machine,
                historical_data,
                current_context
            )

            # Extract peak hours using ML engine
            data_array = ml_engine.prepare_time_series_data(historical_data)
            ml_peak_hours = ml_engine.get_peak_hours_numpy(data_array)

            # Generate 24-hour occupancy forecast using ML insights
            occupancy_forecast = {}
            for i in range(24):
                forecast_hour = (current_hour + i) % 24

                # Use ML forecast confidence and patterns
                forecast_confidence = ml_forecast.get('confidence_score', 50) / 100.0
                base_forecast = current_occupancy_rate

                # Apply ML-learned patterns
                if ml_forecast.get('forecast', {}).get('forecast_usage'):
                    ml_prediction = ml_forecast['forecast']['forecast_usage']
                    # Blend current state with ML prediction
                    base_forecast = base_forecast * 0.4 + ml_prediction * 0.6

                # Add time-based variations learned from data
                hour_offset = abs(forecast_hour - current_hour)
                if hour_offset <= 6:
                    # Use more current data for near-term predictions
                    forecast_occupancy = base_forecast * (1.0 - hour_offset * 0.05)
                else:
                    # Use more historical patterns for longer-term
                    forecast_occupancy = ml_prediction if ml_prediction else base_forecast * 0.8

                # Ensure reasonable bounds
                forecast_occupancy = max(5, min(95, forecast_occupancy))
                occupancy_forecast[f"{forecast_hour}:00"] = round(forecast_occupancy, 1)

            confidence = "high" if ml_forecast.get('confidence_score', 0) > 70 else "medium"

        else:
            print(f"üìâ Insufficient historical data ({len(historical_data)} points), using fallback forecast")
            ml_peak_hours = "Analysis pending"
            occupancy_forecast = {}
            confidence = "low"

            # Simple fallback forecast
            for i in range(6):
                forecast_hour = (current_hour + i) % 24
                forecast_occupancy = max(10, min(80, current_occupancy_rate * (0.9 + i * 0.02)))
                occupancy_forecast[f"{forecast_hour}:00"] = round(forecast_occupancy, 1)

        # Determine current peak status and next peak using ML insights
        is_current_peak = current_occupancy_rate > 60  # High current occupancy indicates peak

        # Find next peak hour from forecast
        next_peak_hour = None
        max_future_occupancy = 0

        for hour_str, occupancy in occupancy_forecast.items():
            hour = int(hour_str.split(':')[0])
            if hour != current_hour and occupancy > max_future_occupancy:
                max_future_occupancy = occupancy
                next_peak_hour = hour

        # Create intelligent peak hours message
        if is_current_peak:
            # Currently experiencing peak - show current status with ML insight
            if isinstance(ml_peak_hours, str) and ml_peak_hours not in ["No data", "No valid data", "Analysis pending"]:
                forecast_message = f"Peak Hours Now ({ml_peak_hours})"
            else:
                forecast_message = f"Peak Activity Now ({current_hour}:00)"
        else:
            # Not in peak - show next forecasted peak
            if next_peak_hour is not None:
                hours_until_peak = (next_peak_hour - current_hour) % 24

                # Generate descriptive name for next peak
                if 6 <= next_peak_hour <= 9:
                    peak_name = "Morning Rush"
                elif 12 <= next_peak_hour <= 14:
                    peak_name = "Lunch Rush"
                elif 17 <= next_peak_hour <= 21:
                    peak_name = "Evening Rush"
                else:
                    peak_name = f"Peak Activity"

                if hours_until_peak == 0:
                    forecast_message = f"Next: {peak_name} (Starting Now)"
                elif hours_until_peak == 1:
                    forecast_message = f"Next: {peak_name} (in 1 hour)"
                elif hours_until_peak <= 6:
                    forecast_message = f"Next: {peak_name} (in {hours_until_peak}h)"
                else:
                    forecast_message = f"Next: {peak_name} ({next_peak_hour}:00)"
            else:
                # Fallback if no clear peak found
                forecast_message = "Peak patterns being analyzed"

        # Prepare response with ML insights
        peak_forecast = {
            'branchId': branch_id,
            'currentHour': current_hour,
            'currentOccupancy': round(current_occupancy_rate, 1),
            'peakHours': forecast_message,
            'confidence': confidence,
            'occupancyForecast': occupancy_forecast,
            'nextPeakIn': (next_peak_hour - current_hour) % 24 if next_peak_hour else 0,
            'totalMachines': total_machines,
            'generatedAt': hk_time.isoformat(),
            'mlAnalysis': {
                'mlPeakHours': ml_peak_hours,
                'dataPoints': len(historical_data),
                'mlEnabled': len(historical_data) >= 50
            }
        }

        print(f"‚úÖ ML-based peak hours forecast generated: {forecast_message} (confidence: {confidence})")

        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps(peak_forecast)
        }

    except Exception as e:
        print(f"‚ùå Error calculating ML peak hours: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', **cors_headers},
            'body': json.dumps({'error': 'Failed to calculate ML peak hours', 'details': str(e)})
        }


def get_branch_coordinates(gym_id):
    """Helper function to get branch coordinates"""
    coordinates = {
        'hk-central': {'lat': 22.2819, 'lon': 114.1577},
        'hk-causeway': {'lat': 22.2783, 'lon': 114.1747}
    }
    return coordinates.get(gym_id, {'lat': 22.2819, 'lon': 114.1577})


# For testing the ML engine independently
if __name__ == "__main__":
    print("ü§ñ GymPulse ML Forecasting Engine - Testing Mode")

    # Test the ML engine
    ml_engine = MLForecastEngine()

    # Mock some test data
    test_context = {
        'machine_id': 'test-machine',
        'status': 'occupied',
        'timestamp': time.time(),
        'category': 'legs',
        'gym_id': 'hk-central'
    }

    # Test with empty data (should trigger fallback)
    result = ml_engine.generate_ai_forecast('test-machine', [], test_context)
    print(f"Test result: {result}")